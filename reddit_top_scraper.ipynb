{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "def scrape_text_from_url(url):\n",
    "    try:\n",
    "        # Send an HTTP request to the URL\n",
    "        response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        \n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            # Parse the HTML content of the page\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Find all elements with the specified class\n",
    "            elements = soup.find_all(\"div\", class_=\"flex flex-col flex-grow items-start\")\n",
    "            \n",
    "            # Initialize lists to store subreddit data\n",
    "            subreddit_data = []\n",
    "            \n",
    "            # Loop through each element and extract data\n",
    "            for element in elements:\n",
    "                subreddit_name = element.find(\"a\", class_=\"m-0 font-bold text-12 text-current truncate max-w-[11rem]\")\n",
    "                community_type = element.find(\"h6\", class_=\"flex-grow h-md text-12 truncate py-[0.125rem] w-[11rem] m-0\")\n",
    "                member_count_elem = element.find(\"h6\", class_=\"text-12 text-neutral-content-weak m-0 truncate w-[11rem]\")\n",
    "                \n",
    "                # Check if elements exist before extracting text\n",
    "                if subreddit_name and community_type and member_count_elem:\n",
    "                    # Extract member count\n",
    "                    member_count = member_count_elem.find(\"faceplate-number\")\n",
    "                    if member_count:\n",
    "                        member_count = member_count['number']\n",
    "                    else:\n",
    "                        member_count = \"Unknown\"\n",
    "                    \n",
    "                    subreddit_data.append({\n",
    "                        \"Subreddit\": subreddit_name.text.strip(),\n",
    "                        \"Community Category\": community_type.text.strip(),\n",
    "                        \"Member Count\": member_count\n",
    "                    })\n",
    "            \n",
    "            return subreddit_data\n",
    "        else:\n",
    "            # If the request was unsuccessful, print an error message\n",
    "            print(\"Error: Unable to retrieve data from URL.\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", e)\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_json(data, filename):\n",
    "    try:\n",
    "        with open(filename, 'w') as json_file:\n",
    "            json.dump(data, json_file, indent=4)\n",
    "        print(f\"Data has been written to {filename} successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while writing to {filename}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://www.reddit.com/best/communities/\"\n",
    "\n",
    "num_pages = 150\n",
    "\n",
    "def scrape_all_tops(base_url, num_pages):\n",
    "    for page_num in tqdm(range(1, num_pages + 1), desc=\"Scraping Pages\"):\n",
    "        url_to_scrape = f\"{base_url}{page_num}\"\n",
    "        scraped_data = scrape_text_from_url(url_to_scrape)\n",
    "        \n",
    "        if scraped_data:\n",
    "            # File name for this page's data\n",
    "            json_filename = f\"top_{page_num}.json\"\n",
    "            \n",
    "            # Write this page's data to a JSON file\n",
    "            write_to_json(scraped_data, json_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "combined_json_filename = \"combined_top_bare.json\"\n",
    "\n",
    "def combine_json_files(folder_path):\n",
    "    combined_data = []\n",
    "\n",
    "    # Get a list of all JSON files in the folder\n",
    "    json_files = [f for f in os.listdir(folder_path) if f.endswith('.json')]\n",
    "\n",
    "    # Sort the list of files\n",
    "    json_files.sort()\n",
    "\n",
    "    # Initialize tqdm with the total number of files\n",
    "    pbar = tqdm(json_files, desc=\"Combining JSON Files\", unit=\"file\")\n",
    "\n",
    "    # Loop through each JSON file\n",
    "    for json_file in pbar:\n",
    "        file_path = os.path.join(folder_path, json_file)\n",
    "        \n",
    "        # Load data from the JSON file\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # Append data to the combined list\n",
    "        combined_data.extend(data)\n",
    "\n",
    "    # Folder path containing the JSON files\n",
    "    folder_path = \"top_bare\"\n",
    "\n",
    "    # Call the function to combine JSON files\n",
    "    combined_data = combine_json_files(folder_path)\n",
    "\n",
    "    # Sort the combined data by 'Member Count'\n",
    "    combined_data.sort(key=lambda x: int(x.get('Member Count', '0')), reverse=True)\n",
    "\n",
    "    # Write the combined data to a JSON file\n",
    "    with open(combined_json_filename, 'w') as json_file:\n",
    "        json.dump(combined_data, json_file)\n",
    "\n",
    "    print(f\"Combined data has been written to {combined_json_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_and_write_filtered_data(input_filename, output_filename):\n",
    "    try:\n",
    "        with open(input_filename, 'r') as json_file:\n",
    "            data = json.load(json_file)\n",
    "        \n",
    "        # Filter the data based on 'Community Type' containing \"politic\" or \"activis\"\n",
    "        filtered_data = []\n",
    "        for item in data:\n",
    "            community_type = item.get('Community Category', '').lower()\n",
    "            if community_type in [\"politics\", \"activism\", \"ethics and philosophy\"] and len(community_type) > 2:\n",
    "                filtered_data.append(item)\n",
    "        \n",
    "        # Write the filtered data to a new JSON file\n",
    "        with open(output_filename, 'w') as output_file:\n",
    "            json.dump(filtered_data, output_file, indent=4)\n",
    "\n",
    "        print(f\"Filtered data has been written to {output_filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "# Output file for filtered data\n",
    "output_politics_filter_filename = \"politics_activism_ethicsphil_top.json\"\n",
    "# Call the function to filter and write filtered data\n",
    "filter_and_write_filtered_data(combined_json_filename, output_politics_filter_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of political subreddits on Reddit's top 150 pages: 764\n",
      "Categories: {'Ethics and Philosophy', 'Activism', 'Politics'}\n",
      "Number of 'Activism' subreddits: 219\n",
      "Number of 'Ethics and Philosophy' subreddits: 162\n",
      "Number of 'Politics' subreddits: 383\n",
      "Largest subreddit: {'Subreddit': 'r/GetMotivated', 'Community Category': 'Ethics and Philosophy', 'Member Count': '21497645'}\n",
      "Smallest subreddit: {'Subreddit': 'r/EndAbuseOfWomenOnline', 'Community Category': 'Activism', 'Member Count': '2984'}\n",
      "Average member count: 137894\n"
     ]
    }
   ],
   "source": [
    "with open(output_politics_filter_filename, 'r') as json_file:\n",
    "    data = json.load(json_file)\n",
    "    categories = {x[\"Community Category\"] for x in data}\n",
    "    print(\"Total number of political subreddits on Reddit's top 150 pages:\", len(data))\n",
    "    print(\"Categories:\", categories)\n",
    "    for category in sorted(categories):\n",
    "        print(f\"Number of '{category}' subreddits:\", len([x for x in data if x[\"Community Category\"] == category]))\n",
    "    print(\"Largest subreddit:\", data[0])\n",
    "    print(\"Smallest subreddit:\", data[-1])\n",
    "    print(\"Average member count:\", round(sum([int(x[\"Member Count\"]) for x in data]) / len(data)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
